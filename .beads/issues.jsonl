{"id":"apollo-0ld","title":"PHASE 2: Design Modular n8n Architecture","description":"Design the modular n8n workflow architecture to replicate PeopleHub functionality.\n\nARCHITECTURE REQUIREMENTS:\n- Natural language query processing\n- LinkedIn profile search and collection\n- Data caching and storage\n- AI research report generation\n- User interface via n8n forms or webhook endpoints\n\nWORKFLOW MODULES TO DESIGN:\n1. Query Parser Workflow (replaces Gemini 2.0 parsing)\n2. Search Execution Workflow (replaces Google Search + scraping)\n3. Data Storage Workflow (replaces PostgreSQL + Redis)\n4. Research Engine Workflow (replaces LangGraph)\n5. Report Generation Workflow\n6. API/Interface Workflow (replaces Next.js API routes)\n\nDESIGN DELIVERABLES:\n- Workflow architecture diagrams\n- Data flow between workflows\n- n8n node specifications\n- Integration points and dependencies\n- Error handling strategies\n- Performance optimization plans\n\nRESEARCH NEEDED:\n- n8n capabilities and limitations\n- Available n8n nodes for each function\n- Data persistence options within n8n\n- AI/LLM integration options in n8n\n\nFOCUS: Modular design that can be implemented incrementally","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T18:01:19.61299-04:00","updated_at":"2025-12-01T18:08:32.585537-04:00","closed_at":"2025-12-01T18:08:32.585537-04:00","labels":["architecture","design","n8n","phase2"],"dependencies":[{"issue_id":"apollo-0ld","depends_on_id":"apollo-e4n","type":"blocks","created_at":"2025-12-01T18:01:38.43457-04:00","created_by":"daemon"}],"comments":[{"id":2,"issue_id":"apollo-0ld","author":"welovekiteboarding","text":"Phase 3 architecture design completed successfully. Created comprehensive modular n8n architecture with 6 core workflows:\n\nCOMPLETED ARCHITECTURE:\n✅ Modular workflow design with clear separation of concerns\n✅ Service integration matrix using free-tier alternatives\n✅ Detailed implementation sequence (5-week plan)\n✅ Cost analysis and risk mitigation strategies\n✅ Performance targets and success metrics\n\nWORKFLOWS DESIGNED:\n1. Query Parser Workflow (replaces Gemini 2.0 with OpenAI GPT-3.5)\n2. Search Engine Workflow (SerpStack + SerpApi fallback)\n3. LinkedIn Data Extraction Workflow (Apify + Proxycurl fallback)\n4. Data Storage Workflow (Google Drive + n8n internal storage)\n5. Research Engine Workflow (multi-step research automation)\n6. Main Search Orchestrator (workflow coordination)\n\nDETAILED IMPLEMENTATION GUIDES:\n✅ Complete node-by-node implementation for Workflow 1 (Query Parser)\n✅ Complete node-by-node implementation for Workflow 2 (Search Engine)\n✅ Input/output contracts, error handling, testing scenarios\n✅ Performance optimization and monitoring strategies\n\nREADY FOR PHASE 4:\n✅ Architecture provides clear implementation roadmap\n✅ All service alternatives validated with free tiers\n✅ Modular design allows incremental development\n✅ Cost-effective approach documented (/bin/zsh-150/month scale)\n\nNext: Begin individual workflow implementation and testing","created_at":"2025-12-01T22:08:25Z"}]}
{"id":"apollo-0yg","title":"REBUILD: Convert PeopleHub Next.js App to n8n Workflows","description":"PROJECT GOAL: Replicate the entire PeopleHub functionality using n8n workflows instead of Next.js, achieving the same end results without using BrightData. Find alternative systems and services with free tiers and no required subscriptions.\n\nCURRENT SYSTEM ANALYSIS:\n- PeopleHub is an AI-powered LinkedIn intelligence platform\n- Uses natural language queries for professional search\n- Combines Google Gemini 2.0 for query parsing, Bright Data APIs for scraping, LangGraph for research workflows\n- Features multi-tier caching (Redis + PostgreSQL) and AI research reports\n- Tech stack: Next.js, Prisma, PostgreSQL, Redis, LangChain, Bright Data\n\nCRITICAL CONSTRAINTS:\n- Use beads for ALL planning and task management (no markdown TODOs)\n- Research and facts ONLY - no guessing or assumptions\n- Modular approach: build one workflow per goal, then integrate\n- Update files frequently to prevent PAI context crashes\n- All work in N8N folder\n- Find free-tier alternatives to BrightData\n\nPHASES:\n1. RESEARCH: Analyze current repo functionality and research alternative services\n2. DESIGN: Plan modular n8n workflow architecture  \n3. IMPLEMENT: Build individual workflows\n4. INTEGRATE: Connect workflows and test end-to-end\n5. DEPLOY: Final deployment and documentation\n\nIMMEDIATE NEXT STEPS:\n1. Document current system architecture and data flows\n2. Research BrightData alternatives with free tiers\n3. Map current features to n8n workflow capabilities\n4. Design modular workflow architecture","status":"open","priority":4,"issue_type":"task","created_at":"2025-12-01T18:00:56.80501-04:00","updated_at":"2025-12-01T18:00:56.80501-04:00","labels":["architecture","project","rebuild"]}
{"id":"apollo-1yn","title":"Document n8n engineering assessment findings","description":"Add engineer's technical feasibility analysis and risk assessment to N8N documentation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T16:57:34.537601-04:00","updated_at":"2025-12-01T16:58:44.758513-04:00","closed_at":"2025-12-01T16:58:44.758513-04:00"}
{"id":"apollo-6m7","title":"Evaluate Apollo.io API as BrightData alternative","description":"Research and evaluate Apollo.io API as alternative to BrightData for this repo. Create integration comparison document.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T15:54:33.228274-04:00","updated_at":"2025-12-01T16:04:15.599059-04:00","closed_at":"2025-12-01T16:04:15.599059-04:00"}
{"id":"apollo-7v5","title":"PHASE 4: Implement Core n8n Workflows","description":"PHASE 4: Implement the core n8n workflows based on completed architecture design\n\nIMPLEMENTATION PRIORITY:\n1. Query Parser Workflow (AI-powered natural language processing)\n2. Search Engine Workflow (SerpStack + SerpApi integration)\n3. LinkedIn Data Extraction Workflow (Apify + Proxycurl)\n4. Data Storage Workflow (Google Drive + caching)\n5. Integration Testing (end-to-end workflow testing)\n\nCORE REQUIREMENTS:\n✅ Set up n8n instance and service accounts\n✅ Implement workflows with detailed node configurations\n✅ Test each workflow independently with sample data\n✅ Validate error handling and fallback mechanisms\n✅ Monitor API usage and performance metrics\n✅ Document workflow performance and limitations\n\nSUCCESS CRITERIA:\n- All workflows functional with free-tier services\n- End-to-end search pipeline working\n- Error handling and fallbacks tested\n- Performance targets met (sub-10 second responses)\n- Cost monitoring active\n\nDEPENDENCIES:\n- Phase 1: System Analysis ✅ COMPLETED\n- Phase 2: Service Research ✅ COMPLETED  \n- Phase 3: Architecture Design ✅ COMPLETED\n\nNEXT PHASE: Phase 5 - Workflow Integration and Testing","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-01T18:08:42.042728-04:00","updated_at":"2025-12-01T18:08:52.950978-04:00","labels":["implementation","n8n","phase4"]}
{"id":"apollo-cpf","title":"Create 3-step plan for watering houseplant","description":"Create a simple 3-step plan for watering a houseplant using the beads issue tracking system","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T17:48:04.166822-04:00","updated_at":"2025-12-01T17:48:18.105026-04:00","closed_at":"2025-12-01T17:48:18.105026-04:00"}
{"id":"apollo-e4n","title":"PHASE 1: Analyze Current System Architecture","description":"Research and document the current PeopleHub system architecture, data flows, and functionality.\n\nTASKS:\n1. Document all API endpoints and their functionality\n2. Map database schema and relationships\n3. Analyze current tech stack dependencies\n4. Document user workflows and data flows\n5. Identify key features that must be replicated\n\nDELIVERABLES:\n- Complete system architecture documentation\n- Data flow diagrams\n- Feature mapping with technical requirements\n- Database schema analysis\n- API documentation summary\n\nFOCUS: Research-based analysis with no assumptions. Document exactly what exists and how it works.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-01T18:01:03.4686-04:00","updated_at":"2025-12-01T18:03:18.81099-04:00","closed_at":"2025-12-01T18:03:18.81101-04:00","labels":["architecture","phase1","research"],"dependencies":[{"issue_id":"apollo-e4n","depends_on_id":"apollo-0yg","type":"blocks","created_at":"2025-12-01T18:01:28.27597-04:00","created_by":"daemon"}],"comments":[{"id":1,"issue_id":"apollo-e4n","author":"welovekiteboarding","text":"Starting comprehensive analysis of current system architecture. Initial findings:\n\nCURRENT API ENDPOINTS ANALYZED:\n1. POST /api/search - Main search functionality with caching\n2. POST/GET /api/research - Research report generation and listing  \n3. GET /api/profiles/recent - Recent profiles API\n4. GET /api/profiles/batch - Batch profile operations\n5. GET /api/profile/[linkedinId]/route.ts - Individual profile data\n6. GET /api/proxy-image - Image proxy for LinkedIn avatars\n\nBRIGHT DATA INTEGRATION IDENTIFIED:\n- Google Search API for LinkedIn URL discovery\n- LinkedIn Scraper API (Dataset ID: gd_l1viktl72bvl7bjuj0)\n- Web Scraper API for research content\n- Polling mechanism with 10-minute timeout\n- JSON response format with comprehensive profile data\n\nLANGGRAPH RESEARCH ENGINE:\n- 8-node workflow: Start → Fetch LinkedIn → Search → Scrape → Summarize → Aggregate → Report\n- Parallel processing capabilities (fan-out/fan-in)\n- State management with MemorySaver\n- AI-powered query generation and content summarization\n- Advanced filtering and deduplication\n\nDATABASE SCHEMA (PostgreSQL):\n- Person model with 22 fields including JSON columns\n- Search model for query caching\n- Research model for report generation\n- Comprehensive indexing strategy\n\nContinuing analysis of tech stack and data flows...","created_at":"2025-12-01T22:02:32Z"}]}
{"id":"apollo-mp5","title":"Redesign repo architecture as n8n workflows","description":"Analyze current repository architecture and redesign as simple n8n workflows to achieve same results. Create N8N folder with workflow files and documentation.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T16:45:28.745871-04:00","updated_at":"2025-12-01T17:01:11.190707-04:00","closed_at":"2025-12-01T17:01:11.190707-04:00"}
{"id":"apollo-q97","title":"RESEARCH: Find BrightData Alternatives with Free Tiers","description":"Research and document alternative services to BrightData that offer genuine free tiers for web scraping and data collection.\n\nBRIGHT DATA SERVICES TO REPLACE:\n- Google Search API\n- LinkedIn Scraper API  \n- Web Scraper API\n\nRESEARCH REQUIREMENTS:\n- Find services with genuine free tiers (not just trials)\n- Verify n8n integration capabilities\n- Document API capabilities and limitations\n- Compare against current Bright Data usage\n- Test actual API access where possible\n\nSERVICES TO RESEARCH:\n1. ScraperAPI\n2. Oxylabs\n3. Smartproxy\n4. ZenRows\n5. ScrapingBee\n6. Rayobyte\n7. Any other free-tier scraping services\n\nDELIVERABLES:\n- Detailed comparison table of services\n- API capability analysis\n- Free tier limitations and requirements\n- n8n integration feasibility\n- Cost analysis for paid tiers\n\nFOCUS: Facts only - test APIs where possible, document exact limitations and capabilities.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-01T18:01:12.053427-04:00","updated_at":"2025-12-01T18:05:36.941458-04:00","closed_at":"2025-12-01T18:05:36.941464-04:00","labels":["alternatives","free-tier","research"],"dependencies":[{"issue_id":"apollo-q97","depends_on_id":"apollo-0yg","type":"blocks","created_at":"2025-12-01T18:01:34.156496-04:00","created_by":"daemon"}]}
{"id":"apollo-sta","title":"Document research functionality preservation analysis","description":"Add researcher's analysis of research capabilities gap in n8n workflows to N8N documentation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T16:57:46.111314-04:00","updated_at":"2025-12-01T16:59:31.952411-04:00","closed_at":"2025-12-01T16:59:31.952411-04:00"}
